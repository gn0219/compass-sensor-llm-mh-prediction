"""
Performance Evaluation Module

Handles classification metrics (accuracy, F1, AUROC) and efficiency metrics (latency, tokens).
"""

import warnings
import numpy as np
import pandas as pd
import json
from typing import Dict, List, Optional
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, balanced_accuracy_score
from .utils import NumpyEncoder

# Suppress sklearn warnings for single-class confusion matrix
warnings.filterwarnings('ignore', message='.*single label.*', category=UserWarning)


def calculate_binary_metrics(y_true: List[int], y_pred: List[int], 
                             y_proba: Optional[List[float]] = None, label_name: str = "") -> Dict:
    """Calculate classification metrics for binary prediction."""
    if len(y_true) == 0:
        return {'accuracy': 0.0, 'balanced_accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0, 
                'auroc': None, 'confusion_matrix': [[0, 0], [0, 0]], 
                'support': {'class_0': 0, 'class_1': 0},
                'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0}
    
    accuracy = accuracy_score(y_true, y_pred)
    balanced_acc = balanced_accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])  # Explicitly specify labels to avoid warnings
    
    # Extract TP, TN, FP, FN from confusion matrix
    # cm is [[TN, FP], [FN, TP]] for binary classification
    if cm.shape == (2, 2):
        tn, fp, fn, tp = cm.ravel()
    elif cm.shape == (1, 1):
        # Only one class present in predictions
        if y_true[0] == y_pred[0] == 0:
            tn, fp, fn, tp = cm[0, 0], 0, 0, 0
        else:
            tn, fp, fn, tp = 0, 0, 0, cm[0, 0]
    else:
        # Handle edge case where only one class is predicted
        tn, fp, fn, tp = 0, 0, 0, 0
        if 0 in y_pred and 0 in y_true:
            tn = cm[0, 0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0
        if 1 in y_true and 0 in y_pred:
            fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)
        if 0 in y_true and 1 in y_pred:
            fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)
        if 1 in y_pred and 1 in y_true:
            tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)
    
    auroc = None
    if y_proba is not None and len(set(y_true)) > 1:
        try:
            auroc = roc_auc_score(y_true, y_proba)
        except Exception as e:
            print(f"Warning: Could not calculate AUROC for {label_name}: {e}")
    
    unique, counts = np.unique(y_true, return_counts=True)
    support = dict(zip([f'class_{int(u)}' for u in unique], counts.tolist()))
    
    return {
        'accuracy': float(accuracy), 'balanced_accuracy': float(balanced_acc),
        'precision': float(precision), 'recall': float(recall), 'f1_score': float(f1),
        'auroc': float(auroc) if auroc else None,
        'confusion_matrix': cm.tolist(), 'support': support,
        'tn': int(tn), 'fp': int(fp), 'fn': int(fn), 'tp': int(tp)
    }


def calculate_mental_health_metrics(results: List[Dict], anxiety_label_key: str = 'phq4_anxiety_EMA',
                                    depression_label_key: str = 'phq4_depression_EMA') -> Dict:
    """Calculate metrics for anxiety and depression predictions."""
    anxiety_true, anxiety_pred, anxiety_proba = [], [], []
    depression_true, depression_pred, depression_proba = [], [], []
    
    for result in results:
        if 'labels' not in result or 'prediction' not in result:
            continue
        
        if anxiety_label_key in result['labels'] and 'Anxiety_binary' in result['prediction']:
            anxiety_true.append(result['labels'][anxiety_label_key])
            anxiety_pred.append(result['prediction']['Anxiety_binary'])
            if 'proba' in result and 'Anxiety' in result['proba']:
                anxiety_proba.append(result['proba']['Anxiety'])
        
        if depression_label_key in result['labels'] and 'Depression_binary' in result['prediction']:
            depression_true.append(result['labels'][depression_label_key])
            depression_pred.append(result['prediction']['Depression_binary'])
            if 'proba' in result and 'Depression' in result['proba']:
                depression_proba.append(result['proba']['Depression'])
    
    anxiety_metrics = calculate_binary_metrics(anxiety_true, anxiety_pred, 
                                               anxiety_proba if anxiety_proba else None, "Anxiety")
    depression_metrics = calculate_binary_metrics(depression_true, depression_pred,
                                                  depression_proba if depression_proba else None, "Depression")
    
    # Calculate macro F1 scores (two different methods)
    # Option 1: Binary-class macro F1 - average F1 of both positive and negative classes per label
    # For each label, calculate F1 for both class 0 and class 1, then average all
    f1_macro_binary = None
    try:
        # Anxiety: F1 for class 0 and class 1
        anxiety_f1_per_class = f1_score(anxiety_true, anxiety_pred, average=None, zero_division=0)
        # Depression: F1 for class 0 and class 1
        depression_f1_per_class = f1_score(depression_true, depression_pred, average=None, zero_division=0)
        # Average all four values
        all_f1_values = list(anxiety_f1_per_class) + list(depression_f1_per_class)
        f1_macro_binary = float(np.mean(all_f1_values))
    except Exception as e:
        print(f"âš ï¸  Warning: Could not calculate f1_macro_binary: {e}")
    
    # Option 2: Multi-label macro F1 - sklearn's standard macro average across all label-sample pairs
    # Treat as multi-label problem: combine anxiety and depression predictions
    f1_macro_multilabel = None
    try:
        # Combine into multi-label format: each sample has 2 labels (anxiety, depression)
        y_true_combined = np.column_stack([anxiety_true, depression_true]).flatten()
        y_pred_combined = np.column_stack([anxiety_pred, depression_pred]).flatten()
        f1_macro_multilabel = float(f1_score(y_true_combined, y_pred_combined, average='macro', zero_division=0))
    except Exception as e:
        print(f"âš ï¸  Warning: Could not calculate f1_macro_multilabel: {e}")
    
    overall_metrics = {
        'accuracy': (anxiety_metrics['accuracy'] + depression_metrics['accuracy']) / 2,
        'balanced_accuracy': (anxiety_metrics['balanced_accuracy'] + depression_metrics['balanced_accuracy']) / 2,
        'precision': (anxiety_metrics['precision'] + depression_metrics['precision']) / 2,
        'recall': (anxiety_metrics['recall'] + depression_metrics['recall']) / 2,
        'f1_score': (anxiety_metrics['f1_score'] + depression_metrics['f1_score']) / 2,
        'f1_macro_binary': f1_macro_binary,  # Option 1: Average F1 of all classes (pos/neg) per label
        'f1_macro_multilabel': f1_macro_multilabel,  # Option 2: sklearn macro average
        'n_samples': len(results)
    }
    
    if anxiety_metrics['auroc'] and depression_metrics['auroc']:
        overall_metrics['auroc'] = (anxiety_metrics['auroc'] + depression_metrics['auroc']) / 2
    else:
        overall_metrics['auroc'] = None
    
    return {'anxiety': anxiety_metrics, 'depression': depression_metrics, 'overall': overall_metrics}


def calculate_classification_metrics(y_true: List[int], y_pred: List[int], target: str = '') -> Dict:
    """Wrapper for classification metrics."""
    return calculate_binary_metrics(y_true, y_pred, None, target)


def calculate_efficiency_metrics(usage_stats: Dict) -> Dict:
    """Calculate efficiency metrics from API usage statistics with standard deviations."""
    num_requests = usage_stats.get('num_requests', 0)
    
    if num_requests == 0:
        return {
            'latency': {'avg_seconds': 0, 'std_seconds': 0, 'total_seconds': 0},
            'cost': {'total_usd': 0, 'per_sample_usd': 0, 'per_sample_std': 0},
            'throughput': {'tokens_per_second': 0, 'samples_per_minute': 0},
            'tokens': {'total': 0, 'prompt': 0, 'completion': 0, 'avg_per_request': 0, 'std_per_request': 0,
                      'prompt_avg': 0, 'prompt_std': 0, 'completion_avg': 0, 'completion_std': 0}
        }
    
    total_latency = usage_stats.get('total_latency', 0)
    total_tokens = usage_stats.get('total_tokens', 0)
    total_cost = usage_stats.get('total_cost', 0)
    prompt_tokens = usage_stats.get('prompt_tokens', 0)
    completion_tokens = usage_stats.get('completion_tokens', 0)
    
    # Get per-request lists for standard deviation calculation
    latencies = usage_stats.get('latencies', [])
    costs = usage_stats.get('costs', [])
    total_tokens_list = usage_stats.get('total_tokens_list', [])
    prompt_tokens_list = usage_stats.get('prompt_tokens_list', [])
    completion_tokens_list = usage_stats.get('completion_tokens_list', [])
    
    # Latency metrics
    avg_latency = total_latency / num_requests
    std_latency = float(np.std(latencies)) if len(latencies) > 1 else 0.0
    
    # Throughput
    tokens_per_second = total_tokens / total_latency if total_latency > 0 else 0
    samples_per_minute = (num_requests / total_latency) * 60 if total_latency > 0 else 0
    requests_per_minute = samples_per_minute  # Same for now
    
    # Cost breakdown
    cost_per_request = total_cost / num_requests
    std_cost = float(np.std(costs)) if len(costs) > 1 else 0.0

    # Token metrics
    avg_tokens_per_request = total_tokens / num_requests
    std_tokens = float(np.std(total_tokens_list)) if len(total_tokens_list) > 1 else 0.0
    avg_prompt = float(np.mean(prompt_tokens_list)) if prompt_tokens_list else 0.0
    std_prompt = float(np.std(prompt_tokens_list)) if len(prompt_tokens_list) > 1 else 0.0
    avg_completion = float(np.mean(completion_tokens_list)) if completion_tokens_list else 0.0
    std_completion = float(np.std(completion_tokens_list)) if len(completion_tokens_list) > 1 else 0.0
    
    return {
        'latency': {
            'avg_seconds': float(avg_latency), 
            'std_seconds': float(std_latency),
            'total_seconds': float(total_latency)
        },
        'cost': {
            'total_usd': float(total_cost), 
            'per_sample_usd': float(cost_per_request),
            'per_sample_std': float(std_cost)
        },
        'throughput': {
            'tokens_per_second': float(tokens_per_second), 
            'samples_per_minute': float(samples_per_minute)
        },
        'tokens': {
            'total': int(total_tokens), 
            'prompt': int(prompt_tokens), 
            'completion': int(completion_tokens), 
            'avg_per_request': float(avg_tokens_per_request),
            'std_per_request': float(std_tokens),
            'prompt_avg': float(avg_prompt),
            'prompt_std': float(std_prompt),
            'completion_avg': float(avg_completion),
            'completion_std': float(std_completion)
        }
    }


def generate_comprehensive_report(results: List[Dict], usage_stats: Dict, config: Optional[Dict] = None) -> Dict:
    """Generate comprehensive performance report with classification and efficiency metrics."""
    classification_metrics = calculate_mental_health_metrics(results)
    efficiency_metrics = calculate_efficiency_metrics(usage_stats)
    
    # Summary statistics
    summary = {
        'total_samples': len(results),
        'total_requests': usage_stats.get('num_requests', 0),
        'overall_accuracy': classification_metrics['overall']['accuracy'],
        'overall_f1': classification_metrics['overall']['f1_score'],
        'total_cost_usd': efficiency_metrics['cost']['total_usd'],
        'avg_latency_sec': efficiency_metrics['latency']['avg_seconds']
    }
    
    return {
        'classification_performance': classification_metrics,
        'cost_efficiency': efficiency_metrics,
        'config': config or {},
        'timestamp': datetime.now().isoformat(),
        'summary': summary
    }


def print_comprehensive_report(report: Dict):
    """Pretty print comprehensive performance report."""
    print("\n" + "="*80)
    print("COMPREHENSIVE PERFORMANCE REPORT")
    print("="*80)
    print(f"\nTimestamp: {report['timestamp']}")
    
    if report.get('config'):
        print("Configuration:")
        for key, value in report['config'].items():
            print(f"  {key}: {value}")
    
    # Summary
    print("\n" + "="*80)
    print("ðŸ“‹ SUMMARY")
    print("="*80)
    summary = report['summary']
    classification = report['classification_performance']
    print(f"  Total Samples:         {summary['total_samples']}")
    print(f"  Total Requests:        {summary['total_requests']}")
    print(f"  Overall Accuracy:      {summary['overall_accuracy']:.4f}")
    print(f"  Overall Bal. Accuracy: {classification['overall']['balanced_accuracy']:.4f}")
    print(f"  Overall F1 Score:      {summary['overall_f1']:.4f}")
    if classification['overall'].get('f1_macro_binary') is not None:
        print(f"  F1 Macro Binary:       {classification['overall']['f1_macro_binary']:.4f}")
    if classification['overall'].get('f1_macro_multilabel') is not None:
        print(f"  F1 Macro Multi:        {classification['overall']['f1_macro_multilabel']:.4f}")
    print(f"  Total Cost:            ${summary['total_cost_usd']:.4f}")
    print(f"  Avg Latency:           {summary['avg_latency_sec']:.2f}s")
    
    # Classification Performance
    print("\n" + "="*80)
    print("7.1 CLASSIFICATION PERFORMANCE")
    print("="*80)
    
    classification = report['classification_performance']
    for target in ['anxiety', 'depression']:
        metrics = classification[target]
        print(f"\nðŸ“Š {target.upper()} PREDICTION")
        print("-" * 80)
        print(f"  Accuracy:          {metrics['accuracy']:.4f}")
        print(f"  Balanced Accuracy: {metrics['balanced_accuracy']:.4f}")
        print(f"  Precision:         {metrics['precision']:.4f}")
        print(f"  Recall:            {metrics['recall']:.4f}")
        print(f"  F1 Score:          {metrics['f1_score']:.4f}")
        print(f"  AUROC:             {metrics['auroc']:.4f}" if metrics['auroc'] else "  AUROC:             N/A")
        print(f"  Support:           {metrics['support']}")
        print(f"  Confusion Matrix: TP={metrics['tp']}, TN={metrics['tn']}, FP={metrics['fp']}, FN={metrics['fn']}")
    
    # Cost & Efficiency
    print("\n" + "="*80)
    print("7.2 COST & EFFICIENCY")
    print("="*80)
    
    efficiency = report['cost_efficiency']
    
    print("\nâ±ï¸  LATENCY")
    print("-" * 80)
    print(f"  Average:       {efficiency['latency']['avg_seconds']:.2f} Â± {efficiency['latency']['std_seconds']:.2f} seconds")
    print(f"  Total:         {efficiency['latency']['total_seconds']:.2f} seconds")
    
    print("\nðŸ’° COST")
    print("-" * 80)
    print(f"  Total:         ${efficiency['cost']['total_usd']:.4f}")
    print(f"  Per Sample:    ${efficiency['cost']['per_sample_usd']:.6f} Â± ${efficiency['cost']['per_sample_std']:.6f}")
    
    print("\nðŸš€ THROUGHPUT")
    print("-" * 80)
    print(f"  Tokens/Second:     {efficiency['throughput']['tokens_per_second']:.2f}")
    print(f"  Samples/Minute:    {efficiency['throughput']['samples_per_minute']:.2f}")
    
    print("\nðŸ”¢ TOKENS")
    print("-" * 80)
    print(f"  Total:         {efficiency['tokens']['total']:,}")
    print(f"  Prompt:        {efficiency['tokens']['prompt']:,} (avg: {efficiency['tokens']['prompt_avg']:.0f} Â± {efficiency['tokens']['prompt_std']:.0f})")
    print(f"  Completion:    {efficiency['tokens']['completion']:,} (avg: {efficiency['tokens']['completion_avg']:.0f} Â± {efficiency['tokens']['completion_std']:.0f})")
    print(f"  Avg/Request:   {efficiency['tokens']['avg_per_request']:.0f} Â± {efficiency['tokens']['std_per_request']:.0f}")
    
    print("\n" + "="*80 + "\n")


def print_metrics_summary(metrics: Dict):
    """Pretty print metrics summary (shorter version)."""
    print("\n" + "="*60)
    print("PERFORMANCE METRICS SUMMARY")
    print("="*60)
    
    for target in ['anxiety', 'depression']:
        m = metrics[target]
        print(f"\nðŸ“Š {target.upper()}")
        print("-" * 60)
        print(f"  Accuracy:          {m['accuracy']:.4f}  |  Precision: {m['precision']:.4f}")
        print(f"  Balanced Accuracy: {m['balanced_accuracy']:.4f}  |  Recall:    {m['recall']:.4f}")
        print(f"  F1 Score:          {m['f1_score']:.4f}", end="")
        if m['auroc']:
            print(f"  |  AUROC:     {m['auroc']:.4f}")
        else:
            print()
    
    print("\nðŸ“Š OVERALL (Macro Average)")
    print("-" * 60)
    overall = metrics['overall']
    print(f"  Accuracy:          {overall['accuracy']:.4f}  |  F1 Score:        {overall['f1_score']:.4f}")
    print(f"  Balanced Accuracy: {overall['balanced_accuracy']:.4f}")
    if overall.get('f1_macro_binary') is not None:
        print(f"  F1 Macro Binary:   {overall['f1_macro_binary']:.4f}  |  F1 Macro Multi:  {overall.get('f1_macro_multilabel', 0):.4f}")
    print("="*60 + "\n")


def metrics_to_dataframe(metrics: Dict) -> pd.DataFrame:
    """Convert metrics dictionary to pandas DataFrame with confusion matrix values."""
    data = {
        'Anxiety': [
            metrics['anxiety']['accuracy'],
            metrics['anxiety']['balanced_accuracy'],
            metrics['anxiety']['precision'],
            metrics['anxiety']['recall'],
            metrics['anxiety']['f1_score'],
            metrics['anxiety']['auroc'] if metrics['anxiety']['auroc'] else 'N/A',
            metrics['anxiety']['tp'],
            metrics['anxiety']['tn'],
            metrics['anxiety']['fp'],
            metrics['anxiety']['fn']
        ],
        'Depression': [
            metrics['depression']['accuracy'],
            metrics['depression']['balanced_accuracy'],
            metrics['depression']['precision'],
            metrics['depression']['recall'],
            metrics['depression']['f1_score'],
            metrics['depression']['auroc'] if metrics['depression']['auroc'] else 'N/A',
            metrics['depression']['tp'],
            metrics['depression']['tn'],
            metrics['depression']['fp'],
            metrics['depression']['fn']
        ],
        'Overall': [
            metrics['overall']['accuracy'],
            metrics['overall']['balanced_accuracy'],
            'N/A',  # precision (not applicable for overall)
            'N/A',  # recall (not applicable for overall)
            metrics['overall']['f1_score'],
            metrics['overall']['auroc'] if metrics['overall']['auroc'] else 'N/A',
            'N/A',  # tp
            'N/A',  # tn
            'N/A',  # fp
            'N/A'   # fn
        ]
    }
    
    index_labels = [
        'Accuracy', 'Balanced Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUROC',
        'TP (True Positive)', 'TN (True Negative)', 'FP (False Positive)', 'FN (False Negative)'
    ]
    
    # Add macro F1 scores if available
    if metrics['overall'].get('f1_macro_binary') is not None:
        data['Overall'].append(metrics['overall']['f1_macro_binary'])
        index_labels.append('F1-Macro-Binary')
    if metrics['overall'].get('f1_macro_multilabel') is not None:
        data['Overall'].append(metrics['overall']['f1_macro_multilabel'])
        index_labels.append('F1-Macro-Multilabel')
        # Fill in N/A for Anxiety and Depression columns
        if len(data['Anxiety']) < len(data['Overall']):
            data['Anxiety'].extend(['N/A'] * (len(data['Overall']) - len(data['Anxiety'])))
        if len(data['Depression']) < len(data['Overall']):
            data['Depression'].extend(['N/A'] * (len(data['Overall']) - len(data['Depression'])))
    
    return pd.DataFrame(data, index=index_labels)


def save_metrics_to_csv(metrics: Dict, filepath: str):
    """Save metrics to CSV file."""
    df = metrics_to_dataframe(metrics)
    df.to_csv(filepath)
    print(f"Metrics saved to: {filepath}")


def export_comprehensive_report(report: Dict, base_filepath: str):
    """Export comprehensive report to JSON and CSV formats."""
    # JSON export with NumpyEncoder to handle DataFrames and numpy types
    json_path = f"{base_filepath}.json"
    with open(json_path, 'w') as f:
        json.dump(report, f, indent=2, cls=NumpyEncoder)
    print(f"âœ… Full report saved: {json_path}")
    
    # Classification CSV
    class_df = metrics_to_dataframe(report['classification_performance'])
    class_path = f"{base_filepath}_classification.csv"
    class_df.to_csv(class_path)
    print(f"âœ… Classification metrics saved: {class_path}")
    
    # Efficiency CSV with standard deviations
    efficiency = report['cost_efficiency']
    eff_data = {
        'Metric': [
            'Avg Latency (s)', 'Std Latency (s)', 'Total Latency (s)', 
            'Total Cost ($)', 'Cost per Sample ($)', 'Std Cost per Sample ($)',
            'Tokens/Second', 'Samples/Minute', 
            'Total Tokens', 'Avg Tokens per Request', 'Std Tokens per Request',
            'Prompt Tokens (Total)', 'Prompt Tokens (Avg)', 'Prompt Tokens (Std)',
            'Completion Tokens (Total)', 'Completion Tokens (Avg)', 'Completion Tokens (Std)'
        ],
        'Value': [
            efficiency['latency']['avg_seconds'], efficiency['latency']['std_seconds'], 
            efficiency['latency']['total_seconds'],
            efficiency['cost']['total_usd'], efficiency['cost']['per_sample_usd'], 
            efficiency['cost']['per_sample_std'],
            efficiency['throughput']['tokens_per_second'], efficiency['throughput']['samples_per_minute'],
            efficiency['tokens']['total'], efficiency['tokens']['avg_per_request'], 
            efficiency['tokens']['std_per_request'],
            efficiency['tokens']['prompt'], efficiency['tokens']['prompt_avg'], 
            efficiency['tokens']['prompt_std'],
            efficiency['tokens']['completion'], efficiency['tokens']['completion_avg'], 
            efficiency['tokens']['completion_std']
        ]
    }
    eff_df = pd.DataFrame(eff_data)
    eff_path = f"{base_filepath}_efficiency.csv"
    eff_df.to_csv(eff_path, index=False)
    print(f"âœ… Efficiency metrics saved: {eff_path}")


def compare_experiments(reports: List[Dict], names: Optional[List[str]] = None) -> pd.DataFrame:
    """Compare multiple experiment reports side-by-side."""
    if names is None:
        names = [f"Experiment {i+1}" for i in range(len(reports))]
    
    comparison_data = {
        'Metric': ['--- Classification ---', 'Anxiety Accuracy', 'Anxiety Balanced Acc', 'Anxiety F1', 
                  'Depression Accuracy', 'Depression Balanced Acc', 'Depression F1', 
                  'Overall Accuracy', 'Overall Balanced Acc', 'Overall F1', 'F1 Macro Binary', 'F1 Macro Multilabel',
                  '--- Efficiency ---', 'Avg Latency (s)', 'Std Latency (s)', 
                  'Total Cost ($)', 'Cost per Sample ($)', 'Std Cost per Sample ($)',
                  'Tokens/Second', 'Samples/Minute', 'Total Tokens', 'Avg Tokens/Request', 'Std Tokens/Request']
    }
    
    for name, report in zip(names, reports):
        class_perf = report['classification_performance']
        efficiency = report['cost_efficiency']
        
        f1_macro_binary = class_perf['overall'].get('f1_macro_binary')
        f1_macro_multilabel = class_perf['overall'].get('f1_macro_multilabel')
        
        comparison_data[name] = [
            '---',
            f"{class_perf['anxiety']['accuracy']:.4f}",
            f"{class_perf['anxiety']['balanced_accuracy']:.4f}",
            f"{class_perf['anxiety']['f1_score']:.4f}",
            f"{class_perf['depression']['accuracy']:.4f}",
            f"{class_perf['depression']['balanced_accuracy']:.4f}",
            f"{class_perf['depression']['f1_score']:.4f}",
            f"{class_perf['overall']['accuracy']:.4f}",
            f"{class_perf['overall']['balanced_accuracy']:.4f}",
            f"{class_perf['overall']['f1_score']:.4f}",
            f"{f1_macro_binary:.4f}" if f1_macro_binary is not None else "N/A",
            f"{f1_macro_multilabel:.4f}" if f1_macro_multilabel is not None else "N/A",
            '---',
            f"{efficiency['latency']['avg_seconds']:.2f}",
            f"{efficiency['latency']['std_seconds']:.2f}",
            f"${efficiency['cost']['total_usd']:.4f}",
            f"${efficiency['cost']['per_sample_usd']:.6f}",
            f"${efficiency['cost']['per_sample_std']:.6f}",
            f"{efficiency['throughput']['tokens_per_second']:.2f}",
            f"{efficiency['throughput']['samples_per_minute']:.2f}",
            f"{efficiency['tokens']['total']:,}",
            f"{efficiency['tokens']['avg_per_request']:.0f}",
            f"{efficiency['tokens']['std_per_request']:.0f}"
        ]
    
    return pd.DataFrame(comparison_data)